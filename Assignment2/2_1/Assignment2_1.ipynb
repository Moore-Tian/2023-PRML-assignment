{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDU PRML 2023 Fall Assignment 2.1\n",
    "\n",
    "Name: <your name>\n",
    "Student ID: <your student id>\n",
    "\n",
    "<font color='red'>**Deadline: 2023-10-30 23:59**</font>\n",
    "<font color='red'>**Overall score weight: 30/100**</font>\n",
    "\n",
    "In this semester, we are going to complete 3 assignments, each may contain **2-3 parts**. This is the first part of the second assignment, in which we will get to know about Pytorch. This assignment contains **little** explorations but **a bunch of** basic operations. So most of us will get full score in this assignment.\n",
    "\n",
    "Installing torch:\n",
    "```bash\n",
    "pip install torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions below and write your torch implementation in the following:\n",
    "\n",
    "## 1. Basic Operations of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_tensor =  torch.zeros(size=(3, 4), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "my_second_tensor = torch.randn(size=(3, 4), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "their_matrix_product = my_first_tensor @ torch.transpose(my_second_tensor, 0, 1)\n",
    "\n",
    "some_meaningless_concatenation = torch.cat([my_first_tensor, my_second_tensor], dim=0)\n",
    "\n",
    "some_meaningless_stack = torch.stack([my_first_tensor]*5, dim=0)\n",
    "# What is the shape of some_meaningless_stack? Can you imagine the geometric interpretation of stacking 5 matrices of shape (3, 4) along the first dimension?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple logistic regression\n",
    "\n",
    "There are 4 core components in Pytorch training process: **model**, **loss function**, **optimizer** and **data loader**. In this part, we will implement a simple logistic regression model to illustrate them.\n",
    "\n",
    "### 2.1 Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear layer for logitstic regression\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear(torch.nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\t\tself.sigmoid = torch.nn.Sigmoid()\n",
    "\t\t\n",
    "  \n",
    "\tdef forward(self, x):\n",
    "\t\toutput = self.linear(x)\n",
    "\t\toutput = self.sigmoid(output)\n",
    "\t\treturn output\n",
    "\n",
    "\n",
    "def loss_function(y_pred, y):\n",
    "    return F.binary_cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data\n",
    "\n",
    "In real world, we usually have to deal with large-scale datasets. However, in this assignment, we will use synthetic data to illustrate the training process. The synthetic data is generated by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data for binary classification\n",
    "\n",
    "num_samples = 100\n",
    "num_features = 2\n",
    "\n",
    "x_0 = torch.randn(num_samples, num_features) + torch.tensor([2.0, 2.0])\n",
    "y_0 = torch.zeros(num_samples)\n",
    "\n",
    "x_1 = torch.randn(num_samples, num_features) + torch.tensor([-2.0, -2.0])\n",
    "y_1 = torch.ones(num_samples)\n",
    "\n",
    "x = torch.cat([x_0, x_1], dim=0)\n",
    "y = torch.cat([y_0, y_1], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset to feed into the model\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x[index], self.y[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\n",
    "dataset = MyDataset(x, y)\n",
    "dataloder = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "my_model = Linear(num_features, 1)\n",
    "\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together\n",
    "\n",
    "Since this is just a toy experiment, we do not need validation.\n",
    "\n",
    "In the following code, we expect to see the training loss decreasing to 0.001 or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.40014904737472534\n",
      "epoch: 10, loss: 0.14905522763729095\n",
      "epoch: 20, loss: 0.05595482140779495\n",
      "epoch: 30, loss: 0.06289394944906235\n",
      "epoch: 40, loss: 0.04849345609545708\n",
      "epoch: 50, loss: 0.026742951944470406\n",
      "epoch: 60, loss: 0.02393188327550888\n",
      "epoch: 70, loss: 0.013506157323718071\n",
      "epoch: 80, loss: 0.016785962507128716\n",
      "epoch: 90, loss: 0.012103969231247902\n",
      "epoch: 100, loss: 0.04488692805171013\n",
      "epoch: 110, loss: 0.012101143598556519\n",
      "epoch: 120, loss: 0.010843245312571526\n",
      "epoch: 130, loss: 0.017391851171851158\n",
      "epoch: 140, loss: 0.0181632898747921\n",
      "epoch: 150, loss: 0.021699152886867523\n",
      "epoch: 160, loss: 0.01942072995007038\n",
      "epoch: 170, loss: 0.010556275025010109\n",
      "epoch: 180, loss: 0.008864075876772404\n",
      "epoch: 190, loss: 0.0011879559606313705\n",
      "epoch: 200, loss: 0.006390029098838568\n",
      "epoch: 210, loss: 0.006628625560551882\n",
      "epoch: 220, loss: 0.008606494404375553\n",
      "epoch: 230, loss: 0.007750355172902346\n",
      "epoch: 240, loss: 0.0065074497833848\n",
      "epoch: 250, loss: 0.012020064517855644\n",
      "epoch: 260, loss: 0.012469933368265629\n",
      "epoch: 270, loss: 0.0034214681945741177\n",
      "epoch: 280, loss: 0.0054931724444031715\n",
      "epoch: 290, loss: 0.011135067790746689\n",
      "epoch: 300, loss: 0.0013491115532815456\n",
      "epoch: 310, loss: 0.011860196478664875\n",
      "epoch: 320, loss: 0.01797356829047203\n",
      "epoch: 330, loss: 0.0037162592634558678\n",
      "epoch: 340, loss: 0.004001731518656015\n",
      "epoch: 350, loss: 0.005673537962138653\n",
      "epoch: 360, loss: 0.004153629764914513\n",
      "epoch: 370, loss: 0.001254882081411779\n",
      "epoch: 380, loss: 0.00651506707072258\n",
      "epoch: 390, loss: 0.006151625420898199\n",
      "epoch: 400, loss: 0.00549811776727438\n",
      "epoch: 410, loss: 0.006783232092857361\n",
      "epoch: 420, loss: 0.006614106707274914\n",
      "epoch: 430, loss: 0.0021149085368961096\n",
      "epoch: 440, loss: 0.009055135771632195\n",
      "epoch: 450, loss: 0.0049789524637162685\n",
      "epoch: 460, loss: 0.0030827075242996216\n",
      "epoch: 470, loss: 0.0028490957338362932\n",
      "epoch: 480, loss: 0.001131778466515243\n",
      "epoch: 490, loss: 0.004738299176096916\n",
      "epoch: 500, loss: 0.001480437582358718\n",
      "epoch: 510, loss: 0.002555401297286153\n",
      "epoch: 520, loss: 0.0014903664123266935\n",
      "epoch: 530, loss: 0.010564470663666725\n",
      "epoch: 540, loss: 0.006091583054512739\n",
      "epoch: 550, loss: 0.0014177185948938131\n",
      "epoch: 560, loss: 0.011617898941040039\n",
      "epoch: 570, loss: 0.0027381370309740305\n",
      "epoch: 580, loss: 0.001662570284679532\n",
      "epoch: 590, loss: 0.0038287879433482885\n",
      "epoch: 600, loss: 0.000702335441019386\n",
      "epoch: 610, loss: 0.00856531597673893\n",
      "epoch: 620, loss: 0.0024316778872162104\n",
      "epoch: 630, loss: 0.01305563747882843\n",
      "epoch: 640, loss: 0.0037987008690834045\n",
      "epoch: 650, loss: 0.001760658691637218\n",
      "epoch: 660, loss: 0.0067083146423101425\n",
      "epoch: 670, loss: 0.0002473318891134113\n",
      "epoch: 680, loss: 0.0017146541504189372\n",
      "epoch: 690, loss: 0.0042451671324670315\n",
      "epoch: 700, loss: 0.0032528003212064505\n",
      "epoch: 710, loss: 0.0009643085650168359\n",
      "epoch: 720, loss: 0.000714918423909694\n",
      "epoch: 730, loss: 0.00518075842410326\n",
      "epoch: 740, loss: 0.0005195606499910355\n",
      "epoch: 750, loss: 0.0004733542737085372\n",
      "epoch: 760, loss: 0.0016774852992966771\n",
      "epoch: 770, loss: 0.0016612509498372674\n",
      "epoch: 780, loss: 0.0007314930553548038\n",
      "epoch: 790, loss: 0.0015786091098561883\n",
      "epoch: 800, loss: 0.0010057931067422032\n",
      "epoch: 810, loss: 0.004655364900827408\n",
      "epoch: 820, loss: 0.004052665550261736\n",
      "epoch: 830, loss: 0.0168419498950243\n",
      "epoch: 840, loss: 0.0029639306012541056\n",
      "epoch: 850, loss: 0.004947346169501543\n",
      "epoch: 860, loss: 0.0021405464503914118\n",
      "epoch: 870, loss: 0.006192948669195175\n",
      "epoch: 880, loss: 0.00020130320626776665\n",
      "epoch: 890, loss: 0.00984048843383789\n",
      "epoch: 900, loss: 0.002248404547572136\n",
      "epoch: 910, loss: 0.0019002301851287484\n",
      "epoch: 920, loss: 0.0014852361055091023\n",
      "epoch: 930, loss: 0.004002372268587351\n",
      "epoch: 940, loss: 0.006227927748113871\n",
      "epoch: 950, loss: 0.004738884046673775\n",
      "epoch: 960, loss: 0.001745815621688962\n",
      "epoch: 970, loss: 0.001280197873711586\n",
      "epoch: 980, loss: 0.00023427626001648605\n",
      "epoch: 990, loss: 0.0028342956211417913\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "\tfor batch_x, batch_y in dataloder:\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tpred = my_model(batch_x)\n",
    "\t\tbatch_y = batch_y.view(-1, 1)\n",
    "\t\tloss = loss_function(pred, batch_y)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tif epoch % 10 == 0:\n",
    "\t\tprint('epoch: {}, loss: {}'.format(epoch, loss.item()))\n",
    "\t\t\n",
    "# save the model\n",
    "\n",
    "torch.save(my_model.state_dict(), 'my_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
